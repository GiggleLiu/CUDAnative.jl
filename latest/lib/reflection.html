<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reflection · CUDAnative.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>CUDAnative.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="../man/usage.html">Usage</a></li><li><a class="toctext" href="../man/troubleshooting.html">Troubleshooting</a></li><li><a class="toctext" href="../man/performance.html">Performance</a></li><li><a class="toctext" href="../man/hacking.html">Hacking</a></li></ul></li><li><span class="toctext">Library</span><ul><li><a class="toctext" href="compilation.html">Compilation &amp; Execution</a></li><li class="current"><a class="toctext" href="reflection.html">Reflection</a><ul class="internal"><li><a class="toctext" href="#Convenience-macros-1">Convenience macros</a></li></ul></li><li><a class="toctext" href="profiling.html">Profiling</a></li><li><span class="toctext">Device Code</span><ul><li><a class="toctext" href="device/intrinsics.html">Intrinsics</a></li><li><a class="toctext" href="device/array.html">Arrays</a></li><li><a class="toctext" href="device/libdevice.html">libdevice</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Library</li><li><a href="reflection.html">Reflection</a></li></ul><a class="edit-page" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/docs/src/lib/reflection.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Reflection</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Reflection-1" href="#Reflection-1">Reflection</a></h1><p>Because of using a different compilation toolchain, CUDAnative.jl offers counterpart functions to the <code>code_</code> functionality from Base:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.code_llvm" href="#CUDAnative.code_llvm"><code>CUDAnative.code_llvm</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">code_llvm([io], f, types; optimize=true, dump_module=false, cap::VersionNumber)</code></pre><p>Prints the LLVM IR generated for the method matching the given generic function and type signature to <code>io</code> which defaults to <code>STDOUT</code>. The IR is optimized according to <code>optimize</code> (defaults to true), and the entire module, including headers and other functions, is dumped if <code>dump_module</code> is set (defaults to false). The device capability <code>cap</code> to generate code for defaults to the current active device&#39;s capability, or v&quot;2.0&quot; if there is no such active context.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L26-L35">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.code_ptx" href="#CUDAnative.code_ptx"><code>CUDAnative.code_ptx</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">code_ptx([io], f, types; cap::VersionNumber, kernel::Bool=false)</code></pre><p>Prints the PTX assembly generated for the method matching the given generic function and type signature to <code>io</code> which defaults to <code>STDOUT</code>. The device capability <code>cap</code> to generate code for defaults to the current active device&#39;s capability, or v&quot;2.0&quot; if there is no such active context. The optional <code>kernel</code> parameter indicates whether the function in question is an entry-point function, or a regular device function.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L54-L62">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.code_sass" href="#CUDAnative.code_sass"><code>CUDAnative.code_sass</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">code_sass([io], f, types, cap::VersionNumber)</code></pre><p>Prints the SASS code generated for the method matching the given generic function and type signature to <code>io</code> which defaults to <code>STDOUT</code>. The device capability <code>cap</code> to generate code for defaults to the current active device&#39;s capability, or v&quot;2.0&quot; if there is no such active context.</p><p>Note that the method needs to be a valid entry-point kernel, ie. it should not return any values.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L76-L86">source</a><br/></section><h2><a class="nav-anchor" id="Convenience-macros-1" href="#Convenience-macros-1">Convenience macros</a></h2><p>For ease of use, CUDAnative.jl also implements <code>@code_</code> macros wrapping the above reflection functionality. These macros determines the type of arguments (taking into account GPU type conversions), and call the underlying <code>code_</code> function. In addition, these functions understand the <code>@cuda</code> invocation syntax, so you conveniently put them in front an existing <code>@cuda</code> invocation.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_lowered" href="#CUDAnative.@code_lowered"><code>CUDAnative.@code_lowered</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_lowered</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_lowered on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_typed" href="#CUDAnative.@code_typed"><code>CUDAnative.@code_typed</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_typed</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_typed on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_warntype" href="#CUDAnative.@code_warntype"><code>CUDAnative.@code_warntype</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_warntype</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_warntype on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_llvm" href="#CUDAnative.@code_llvm"><code>CUDAnative.@code_llvm</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_llvm</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_llvm on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_ptx" href="#CUDAnative.@code_ptx"><code>CUDAnative.@code_ptx</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_ptx</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_ptx on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="CUDAnative.@code_sass" href="#CUDAnative.@code_sass"><code>CUDAnative.@code_sass</code></a> — <span class="docstring-category">Macro</span>.</div><div><pre><code class="language-none">code_sass</code></pre><p>Extracts the relevant function call from any <code>@cuda</code> invocation, evaluates the arguments to the function or macro call, determines their types (taking into account GPU-specific type conversions), and calls code_sass on the resulting expression. Can be applied to a pure function call, or a call prefixed with the <code>@cuda</code> macro. In that case, kernel code generation conventions are used (wrt. argument conversions, return values, etc).</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaGPU/CUDAnative.jl/tree/54e1b9068dfe500b5472ce64d04e74fe0a5217d0/src/reflection.jl#L118-L126">source</a><br/></section><footer><hr/><a class="previous" href="compilation.html"><span class="direction">Previous</span><span class="title">Compilation &amp; Execution</span></a><a class="next" href="profiling.html"><span class="direction">Next</span><span class="title">Profiling</span></a></footer></article></body></html>
